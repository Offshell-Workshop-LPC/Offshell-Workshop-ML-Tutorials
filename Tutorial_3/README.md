# Tutorial 3 - Introduction to Regressor BDT
This tutorial is developed for the Offshell Workshop - Friday 10:25-11:25.
[Introduction Slides](https://indico.cern.ch/event/1375252/timetable/#18-machine-learning)


## Getting Setup

```
cd Tutorial_3
```

## Imports
The first step is to import the packages we will need to create the BDT.
```
# General Utilities
import math
import random
import numpy as np
import uproot

# Sklearn Utilities
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error

# Plotting Utilities
import matplotlib
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import matplotlib.pyplot as plt
```
## Initializing Variables
We also need to initialize some variables we want to use in our script.

```
### Initializing Input File Information

INPUT_FILE_NAME = "../rootfiles/EMTF_ntuple_slimmed.root"

### Initializing Input Variable Information

INPUT_VAR_NAMES = ["theta", "st1_ring2", "dPhi_12", "dPhi_23", "dPhi_34", "dPhi_13", "dPhi_14", "dPhi_24", "FR_1", "bend_1", "dPhiSum4", "dPhiSum4A", "dPhiSum3", "dPhiSum3A", "outStPhi", "dTh_14", "RPC_1", "RPC_2", "RPC_3", "RPC_4"]

### Initializing empty X, Y, W

X = []
Y = []
W = []
```

## Reading Root Files using Uproot
We will be using Uproot to read our input root files. Uproot is very useful because it makes the interface with ML tools much easier since branches are converted to numpy arrays. This makes the process of reading in our data very straight forward - we can simply loop through each file and pulling the input variables directly. One thing to be mindful of is that we need our `X` array to be in the proper format, so it should be that each element in the 2D array should correspond to the input layer but when we get these variables our of the root file using `f["tree/" + var].array()` each element will be for a given variable. Thus, we will need to take the transpose of the `input_var` array.

Example:
```
input_var_directly = [ [pt1, pt2, pt3, pt4 ...],
                       [mass1, mass2, mass3, mass4 ...]
                       [eta1, eta2, eta3, eta4 ...] ]

input_var_transposed  = [ [pt1, mass1, eta1],
                          [pt2, mass2, eta2],
                          [pt3, mass3, eta3],
                          [pt4, mass4, eta4], ...]
```

```

### Reading Root Files and Filling X, Y, W

f = uproot.open(INPUT_FILE_NAME)
input_vars = [f["tree/" + var].array() for var in INPUT_VAR_NAMES]

X = np.transpose(input_vars)
Y = f["tree/GEN_pt"].array()
W = [1]*len(input_vars[0])
```

## Defining the XGBoost Model
We can define our `XGBRegressor` by passing a loss function, learning rate, max tree depth, forest size, and number of threads to parallelize. The `xg_reg` variable holds the BDT model and can be trained by calling `xg_reg.fit()`. We can obtain the results of the training by calling `xg_reg.eval_result()` and get the values of the metric we defined in `eval_metric`.

```

### Splitting into Training and Testing Sets
X_train, X_test, Y_train, Y_test, W_train, W_test = train_test_split(X, Y, W, test_size=.5, random_state=123)

X_train_split, X_val, Y_train_split, Y_val, W_train_split, W_val = train_test_split(X_train, Y_train, W_train, test_size=.5, random_state=321)


### Defining our XGBoost Model

xg_reg = xgb.XGBRegressor(objective = 'reg:squarederror', 
                        learning_rate = .15, 
                        max_depth = 3, 
                        n_estimators = 400,
                        nthread = 30)

xg_reg.fit(X_train_split, Y_train_split, eval_metric=["rmse"], sample_weight = W_train_split, eval_set=[(X_train_split, Y_train_split), (X_val, Y_val)])

results = xg_reg.evals_result()

```

## Plotting Performance

Looking at the RMSE can be helpful for identifying problems in the training. It can be a good indicator of overfitting, incorrect targets, or incorrect event weighting.

```
pdf_pages = PdfPages("./bdt_history_tutorial3.pdf")

fig, ax = plt.subplots(1)
fig.suptitle("Model Loss")
ax.plot(results['validation_0']['rmse'])
ax.plot(results['validation_1']['rmse'])
ax.set_ylabel('RMSE')
ax.set_xlabel('Epoch')
ax.legend(['train','test'], loc='upper left')
fig.set_size_inches(6,6)

pdf_pages.savefig(fig)
```

Resolution is a good metric to see how well the regressor is performing. The resolution plot is generated by the proportion of error and provides a snapshot into how accurate the regresor is compared to the true value. The resolution plot also shows the expected spread of values and can indicate biases in the training.

```
predictions = xg_reg.predict(X_test)
predictions_train = xg_reg.predict(X_train_split)

resolution = [(Y_test[i] - predictions[i])/Y_test[i] for i in range(0,len(Y_test))]
resolution_train = [(Y_train_split[i] - predictions_train[i])/Y_train_split[i] for i in range(0,len(Y_train_split))]

res_binned, res_bins = np.histogram(resolution, 100, (-2,2), density=True)
res_binned_train, res_bins = np.histogram(resolution_train, 100, (-2,2),density=True)

fig2, ax2 = plt.subplots(1)
fig2.suptitle("Model Resolution")
ax2.errorbar([res_bins[i]+(res_bins[i+1]-res_bins[i])/2 for i in range(0, len(res_bins)-1)],
                    res_binned, xerr=[(res_bins[i+1] - res_bins[i])/2 for i in range(0, len(res_bins)-1)],
                    linestyle="", marker=".", markersize=3, elinewidth = .5,label="Test")

ax2.errorbar([res_bins[i]+(res_bins[i+1]-res_bins[i])/2 for i in range(0, len(res_bins)-1)],
                    res_binned_train, xerr=[(res_bins[i+1] - res_bins[i])/2 for i in range(0, len(res_bins)-1)],
                    linestyle="", marker=".", markersize=3, elinewidth = .5,label="Train")

ax2.set_ylabel('$N_{events}$')
ax2.set_xlabel("$(p_T^{GEN} - p_T^{BDT})/(p_T^{GEN})$")
ax2.legend()
fig2.set_size_inches(6,6)


pdf_pages.savefig(fig2)

pdf_pages.close()

```

## Exercises
*Change the target variable to the log of pT to see a huge increase in performance. Remember that the prediction will also be logged so to make the resolution plot it will need to be `np.exp(predictions)`*


